# Servers.
conductor.jetty.server.enabled=true
conductor.grpc.server.enabled=false

# Database persistence model.  Possible values are memory, redis, and dynomite.
# If ommitted, the persistence used is memory
#
# memory : The data is stored in memory and lost when the server dies.  Useful for testing or demo
# redis : non-Dynomite based redis instance
# dynomite : Dynomite cluster.  Use this for HA configuration.

db=dynomite

# Dynomite Cluster details.
# format is host:port:rack separated by semicolon
workflow.dynomite.cluster.hosts=dyno1:8102:us-east-1c

# Dynomite cluster name
workflow.dynomite.cluster.name=dyno1

# Namespace for the keys stored in Dynomite/Redis
workflow.namespace.prefix=conductor

# Namespace prefix for the dyno queues
workflow.namespace.queue.prefix=conductor_queues

# No. of threads allocated to dyno-queues (optional)
queues.dynomite.threads=10

# By default with dynomite, we want the repairservice enabled
workflow.repairservice.enabled=true

# Non-quorum port used to connect to local redis.  Used by dyno-queues.
# When using redis directly, set this to the same port as redis server
# For Dynomite, this is 22122 by default or the local redis-server port used by Dynomite.
queues.dynomite.nonQuorum.port=22122

# Elastic search instance type. Possible values are memory and external.
# If not specified, the instance type will be embedded in memory
#
# memory: The instance is created in memory and lost when the server dies. Useful for development and testing.
# external: Elastic search instance runs outside of the server. Data is persisted and does not get lost when
#           the server dies. Useful for more stable environments like staging or production.
workflow.elasticsearch.instanceType=external

# Transport address to elasticsearch
workflow.elasticsearch.url=es:9300

# Name of the elasticsearch cluster
workflow.elasticsearch.index.name=conductor

# Additional modules (optional)
# conductor.additional.modules=class_extending_com.google.inject.AbstractModule

# Additional modules for metrics collection (optional)
# conductor.additional.modules=com.netflix.conductor.contribs.metrics.MetricsRegistryModule,com.netflix.conductor.contribs.metrics.LoggingMetricsModule
# com.netflix.conductor.contribs.metrics.LoggingMetricsModule.reportPeriodSeconds=15

# Load sample kitchen sink workflow
loadSample=true

#uncomment to enable kafka based event features.
conductor.additional.modules=com.netflix.conductor.contribs.kafka.KafkaModule
conductor.kafka.listener.enabled=true
conductor.kafka.workers.listener.enabled=true

#all main kafka properties are suffixed "kafka."
# **** Add cap. to set env dynamically  *****
kafka.bootstrap.servers=kafka-dev.darwin.int.gliaecosystems.com:9091
#all kafka admin properties are suffixed "admin.kafka."
admin.kafka.client.id=KafkaTopicManager
#kafka topics manager properties
topic.manager.kafka.request.timeout.ms=10000
topic.manager.kafka.num.of.partitions=3
topic.manager.kafka.num.of.replications=1
#all kafka consumer properties are suffixed "consumer.kafka."
conductor.kafka.consumer.listener.topic=Apex_Requests
consumer.kafka.group.id=KafkaConsumer
consumer.kafka.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
consumer.kafka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
# Set the number of messages read by a kafka consumer during a single poll (Default = 500)
consumer.kafka.max.poll.records=10
#all kafka producer properties are suffixed "producer.kafka."
conductor.kafka.producer.listener.topic=Apex_Response
producer.kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
producer.kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer
#all kafka streams properties are suffixed "streams.kafka."
streams.kafka.application.id=Apex
streams.kafka.num.stream.threads=1
# production exception handler kafka steams configs
conductor.kafka.streams.production.exception.handler.retry.count = 3
conductor.kafka.streams.production.exception.handler.retry.delay.ms = 5000
conductor.kafka.streams.production.exception.handler.thread.pool = 10
# workers kafka steams configs
conductor.kafka.streams.workers.consumer.listener.topic=Register_Service_Request
conductor.kafka.streams.workers.producer.listener.topic=Register_Service_Response
conductor.kafka.listener.startup.thread.sleep = 50000
conductor.kafka.workers.listener.thread.pool=30
conductor.kafka.workers.listener.poll.batch.size=30
# listener kafka streams configs
conductor.kafka.listener.thread.pool=10
conductor.kafka.workers.listener.startup.thread.sleep = 45000

# Conductor heartbeat configs
heartbeat.topic=Heartbeat
heartbeat.interval.ms=45000
workers.heartbeat.topic=Workers_Heartbeat
worker.inactive.ms=120000

# cache.max.bytes.buffering controls the amount of memory
# allocated (spilt between threads) for KTable (state-stores)
# Default 10 MB
#kafka.streams.cache.max.bytes.buffering=
# commit.interval.ms controls the frequency of the state being saved
# Default 30 secs
#kafka.streams.commit.interval.ms=
#Following properties set for using AMQP events and tasks with conductor:
#conductor.additional.modules=com.netflix.conductor.contribs.AMQPModule(You must add module AMQModule to enable support of AMQP queues)

