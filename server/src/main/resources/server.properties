#
# Copyright 2017 Netflix, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# uncomment below if using local Redis.
# if commented, a memory db will be used by default
# db=redis

#Dynomite Cluster details.
#format is host:port:rack separated by semicolon
# uncomment below if not using local Redis
workflow.dynomite.cluster.hosts=host1:port:rack;host2:port:rack:host3:port:rack
# uncomment below if using local Redis.
# workflow.dynomite.cluster.hosts=apex-nonprod-redis.7vudlw.ng.0001.euw1.cache.amazonaws.com:6379:eu-west-1b

# Dynomite cluster name
workflow.dynomite.cluster.name=conductor

#namespace for the keys stored in Dynomite/Redis
workflow.namespace.prefix=conductor

#namespace prefix for the dyno queues
workflow.namespace.queue.prefix=conductor

#no. of threads allocated to dyno-queues
queues.dynomite.threads=10
workflow.dynomite.connection.maxConnsPerHost=31

# By default with dynomite, we want the repairservice enabled
workflow.repairservice.enabled=true


#non-quorum port used to connect to local redis.  Used by dyno-queues
# uncomment below if not using local Redis
queues.dynomite.nonQuorum.port=22122
# uncomment below if using local Redis
# queues.dynomite.nonQuorum.port=6379

#Transport address to elasticsearch
workflow.elasticsearch.url=localhost:9300

#Name of the elasticsearch cluster
workflow.elasticsearch.index.name=conductor

#Elasticsearch major release version.
workflow.elasticsearch.version=2

# For a single node dynomite or redis server, make sure the value below is set to same as rack specified in the "workflow.dynomite.cluster.hosts" property.
# Uncomment below if not using local Redis
EC2_AVAILABILITY_ZONE=us-east-1c
EC2_REGION=us-east-1
# Uncomment below if using local Redis
#EC2_AVAILABILITY_ZONE=eu-west-1b
#EC2_REGION=eu-west-1

# disable archival service
workflow.archive=false

#zookeeper
# workflow.zookeeper.lock.connection=host1.2181,host2:2181,host3:2181
# workflow.zookeeper.lock.sessionTimeoutMs
# workflow.zookeeper.lock.connectionTimeoutMs

#enable locking during workflow execution
workflow.decider.locking.enabled=false
workflow.decider.locking.namespace=
workflow.decider.locking.server=noop_lock
workflow.decider.locking.leaseTimeInSeconds=60

#Redis cluster settings for locking module
# workflow.redis.locking.server.type=single
#Comma separated list of server nodes
# workflow.redis.locking.server.address=redis://127.0.0.1:6379
#Redis sentinel master name
# workflow.redis.locking.server.master.name=master

#uncomment to enable kafka based event features.
conductor.additional.modules=com.netflix.conductor.contribs.kafka.KafkaModule
conductor.kafka.listener.enabled=true
conductor.kafka.workers.listener.enabled=true

#all main kafka properties are suffixed "kafka."
# **** Add cap. to set env dynamically  *****
kafka.bootstrap.servers=b-1.apex-nonprod-kafka.k6xf72.c3.kafka.eu-west-1.amazonaws.com:9092,b-2.apex-nonprod-kafka.k6xf72.c3.kafka.eu-west-1.amazonaws.com:9092,b-3.apex-nonprod-kafka.k6xf72.c3.kafka.eu-west-1.amazonaws.com:9092
# kafka.bootstrap.servers=kafka-dev.darwin.int.gliaecosystems.com:9091
# kafka.bootstrap.servers=localhost:9092
#all kafka admin properties are suffixed "admin.kafka."
admin.kafka.client.id=KafkaTopicManager
#kafka topics manager properties
topic.manager.kafka.request.timeout.ms=10000
topic.manager.kafka.num.of.partitions=3
topic.manager.kafka.num.of.replications=3
#all kafka consumer properties are suffixed "consumer.kafka."
conductor.kafka.consumer.listener.topic=Apex_Requests
consumer.kafka.group.id=KafkaConsumer
consumer.kafka.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
consumer.kafka.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
consumer.kafka.partition.assignment.strategy=org.apache.kafka.clients.consumer.RoundRobinAssignor
# Set the number of messages read by a kafka consumer during a single poll (Default = 500)
# consumer.kafka.max.poll.records=10
#all kafka producer properties are suffixed "producer.kafka."
conductor.kafka.producer.listener.topic=Apex_Response
producer.kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
producer.kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer
producer.kafka.batch.size=0
producer.kafka.linger.ms=0
producer.kafka.enable.idempotence=true
producer.kafka.acks=all
producer.kafka.max.in.flight.requests.per.connection=5
producer.kafka.delivery.timeout.ms=240000
producer.kafka.retries=10
# producer.kafka.partitioner.class=org.apache.kafka.clients.producer.RoundRobinPartitioner
#all kafka streams properties are suffixed "streams.kafka."
streams.kafka.application.id=Apex
streams.kafka.num.stream.threads=1
# production exception handler kafka steams configs
conductor.kafka.streams.production.exception.handler.retry.count = 3
conductor.kafka.streams.production.exception.handler.retry.delay.ms = 5000
conductor.kafka.streams.production.exception.handler.thread.pool = 10
# workers kafka steams configs
conductor.kafka.streams.workers.consumer.listener.topic=Register_Service_Request
conductor.kafka.streams.workers.producer.listener.topic=Register_Service_Response
conductor.kafka.listener.startup.thread.sleep = 50000
conductor.kafka.workers.task.stream.factory.thread.pool=30
conductor.kafka.workers.task.stream.poll.batch.size=30
conductor.kafka.workers.task.stream.task.polling.interval = 1
# listener kafka streams configs
conductor.kafka.listener.thread.pool=10
conductor.kafka.listener.workflow.status.monitor.polling.interval = 1
conductor.kafka.workers.listener.startup.thread.sleep = 45000

# Conductor heartbeat configs
heartbeat.topic=Heartbeat
#heartbeat.interval.ms=45000
heartbeat.interval.ms=3000
workers.heartbeat.topic=Workers_Heartbeat
#worker.inactive.ms=60000
worker.inactive.ms=10000

# cache.max.bytes.buffering controls the amount of memory
# allocated (spilt between threads) for KTable (state-stores)
# Default 10 MB
#kafka.streams.cache.max.bytes.buffering=
# commit.interval.ms controls the frequency of the state being saved
# Default 30 secs
#kafka.streams.commit.interval.ms=
#Following properties set for using AMQP events and tasks with conductor:
#conductor.additional.modules=com.netflix.conductor.contribs.AMQPModule(You must add module AMQModule to enable support of AMQP queues)


# Here are the settings with default values:
#workflow.event.queues.amqp.hosts=<rabbitmq serverip>
#workflow.event.queues.amqp.username=<username>
#workflow.event.queues.amqp.password=<password>



#workflow.event.queues.amqp.virtualHost=/
#workflow.event.queues.amqp.port=5672
#workflow.event.queues.amqp.useNio=false
#workflow.event.queues.amqp.batchSize=1

#workflow.event.queues.amqp.pollTimeInMs=100

#workflow.listener.queue.useExchange=true( exchange or queue)
#workflow.listener.queue.prefix=myqueue
# Use durable queue ?
#workflow.event.queues.amqp.durable=false
# Use exclusive queue ?
#workflow.event.queues.amqp.exclusive=false
# Enable support of priorities on queue. Set the max priority on message.
# Setting is ignored if the value is lower or equals to 0
# workflow.event.queues.amqp.maxPriority=-1

